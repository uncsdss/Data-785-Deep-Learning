{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a74f39b",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54a256",
   "metadata": {},
   "source": [
    "## Text Representations\n",
    "\n",
    "In machine learning and neural networks, the data often needs to be numerical since models cannot usually work directly with raw text. Text is inherently complex, filled with semantics, context, and structure that computers cannot easily interpret. To train models effectively on textual data, we need systematic ways to convert text into numerical representations while preserving as much relevant information as possible. Without this transformation, machine learning algorithms would be unable to derive patterns or make predictions from text data.\n",
    "\n",
    "Text is composed of characters and words, which hold meaning for humans but are not directly understandable by machines. To perform tasks like classification, clustering, or generation, machine learning models need features, or numerical inputs, that capture relevant patterns in the text. \n",
    "\n",
    "Effective text representations help models understand and make predictions by encoding:\n",
    "- Frequency and presence of important words\n",
    "- Context and relationships between words\n",
    "- Semantic meaning beyond the surface structure\n",
    "\n",
    "The quality of these representations directly impact model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1dda45",
   "metadata": {},
   "source": [
    "## Different Approaches To Text Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67bfb3",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)  \n",
    "\n",
    "The Bag of Words approach is one of the simplest and most widely used text representations. It involves creating a vocabulary from the text and representing each document as a vector indicating the presence or frequency of each word in the vocabulary.\n",
    "\n",
    "For example, consider two sentences:  \n",
    "- \"The cat sat on the mat.\"  \n",
    "- \"The dog sat on the mat.\"  \n",
    "\n",
    "The vocabulary from these sentences is: `['the', 'cat', 'dog', 'sat', 'on', 'mat']`. \n",
    "The first sentence can be represented as a vector: `[2, 1, 0, 1, 1, 1]`, where each entry corresponds to the count of a word from the vocabulary. \n",
    "\n",
    "While BoW is easy to implement, it has limitations. Primarily, it ignores word order and the context in which words appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe0a88",
   "metadata": {},
   "source": [
    "![](https://aiml.com/wp-content/uploads/2023/02/disadvantage-bow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca3be9",
   "metadata": {},
   "source": [
    "#### Example Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ab02b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>at</th>\n",
       "      <th>ball</th>\n",
       "      <th>barked</th>\n",
       "      <th>cat</th>\n",
       "      <th>chased</th>\n",
       "      <th>dog</th>\n",
       "      <th>mat</th>\n",
       "      <th>mouse</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   at  ball  barked  cat  chased  dog  mat  mouse  on  sat  the\n",
       "0   0     0       0    1       0    0    1      0   1    1    2\n",
       "1   1     0       1    1       0    1    0      0   0    0    2\n",
       "2   0     0       0    1       1    0    0      1   0    0    2\n",
       "3   0     1       0    0       1    1    0      0   0    0    2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog barked at the cat\",\n",
    "    \"The cat chased the mouse\",\n",
    "    \"The dog chased the ball\"\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus into BoW representation\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the result to a DataFrame for readability\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the Bag of Words representation\n",
    "print(\"Bag of Words Representation:\")\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a14a6",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency)  \n",
    "\n",
    "TF-IDF is a statistical measure used to evaluate how important a word (term) is to a document within a collection or corpus of documents. It builds upon the simple Bag of Words (BoW) representation by incorporating both the frequency of a term in a single document and how common (or rare) that term is across the entire corpus. In other words, TF-IDF helps us down-weight very common words (like “the”, “is”, “and”) that carry little distinguishing information and up-weight rarer and more informative words.\n",
    "\n",
    "#### Term Frequency (TF)\n",
    "\n",
    "The term frequency component quantifies how often a given term appears in a specific document. A higher TF value means the term appears more often in that document, suggesting it may be important within that document’s context. There are a few common ways to define TF:\n",
    "\n",
    "- **Raw Count**:  \n",
    "  $\\mathrm{TF}(t, d) = \\text{count of term } t \\text{ in document } d.$\n",
    "  \n",
    "\n",
    "- **Normalized Frequency** (accounts for document length):  \n",
    "  $\\mathrm{TF}(t, d) = \\frac{\\text{count of } t \\text{ in } d}{\\text{total words in } d}.$\n",
    "  \n",
    "\n",
    "- **Log-Scaled Frequency** (dampens large counts):  \n",
    "  $\\mathrm{TF}(t, d) =\n",
    "    \\begin{cases}\n",
    "      1 + \\log(\\text{count of } t \\text{ in } d), & \\text{if count} > 0,\\\\\n",
    "      0, & \\text{otherwise}.\n",
    "    \\end{cases}$\n",
    "    \n",
    "\n",
    "#### Inverse Document Frequency (IDF)\n",
    "\n",
    "While TF measures how often a term appears in one document, IDF measures how unique or rare that term is across the entire corpus. Intuitively:\n",
    "\n",
    "- If a term appears in almost every document (e.g., “the” or “and”), it carries less discriminative power.\n",
    "- If a term appears in very few documents (e.g., “photosynthesis” in a mixed corpus), it is more informative.\n",
    "\n",
    "A common definition of IDF is:\n",
    "\n",
    "$$\n",
    "\\mathrm{IDF}(t, D) \\;=\\; \\log\\!\\Bigl(\\frac{N}{\\,|\\{d \\in D: t \\in d\\}|\\,}\\Bigr),\n",
    "$$  \n",
    "\n",
    "where:\n",
    "\n",
    "- $N$ is the total number of documents in the corpus $D$.\n",
    "- $|\\{\\,d \\in D : t \\in d\\,\\}|$ is the number of documents in which term $t$ appears (the document frequency of $t$).\n",
    "\n",
    "\n",
    "Some variants add 1 to the denominator (or to the log) to avoid division by zero or negative values:  \n",
    "$$\n",
    "\\mathrm{IDF}(t, D) = \\log\\!\\Bigl(\\frac{1+N}{1 + |\\{d: t \\in d\\}|}\\Bigr) + 1.\n",
    "$$\n",
    "\n",
    "The higher the IDF, the rarer the term is across documents.\n",
    "\n",
    "#### Combining TF and IDF\n",
    "\n",
    "The core idea of TF-IDF is to multiply the two components:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d, D) \\;=\\; \\text{TF}(t, d) \\times \\text{IDF}(t, D).\n",
    "$$\n",
    "\n",
    "- If a term $t$ occurs frequently in a document $d$ (high TF) but rarely in the rest of the corpus (high IDF), then TF-IDF is large.  \n",
    "- If a term is common in a document but also common across many documents (low IDF), then TF-IDF remains moderate.  \n",
    "- If a term is very rare in the specific document (low TF), its TF-IDF is low regardless of its IDF.  \n",
    "\n",
    "By weighting words this way, we produce a vector representation for each document where each dimension corresponds to a vocabulary term, and the value is the TF-IDF score. These vectors can then be used as features in downstream tasks (classification, clustering, information retrieval, etc.).\n",
    "\n",
    "#### Limitations of TF-IDF\n",
    "\n",
    "- **Ignores Word Order**: TF-IDF still treats each term independently and does not capture the sequence or context in which words appear. For example, “dog bites man” and “man bites dog” have the same TF-IDF representation even though their meanings differ.\n",
    "\n",
    "- **Ignores Word Relationships/Semantics**: Synonyms (“car” vs. “automobile”) will be treated as completely separate dimensions. TF-IDF does not capture semantic similarity between related words.\n",
    "\n",
    "- **High Dimensionality**: The resulting feature space is as large as the vocabulary size, which can be in the tens or hundreds of thousands. Sparse, high-dimensional feature vectors can be computationally expensive to store and process.\n",
    "\n",
    "- **Static Across Corpus**: Once you compute IDF, those values remain fixed unless you retrain. In dynamic corpora where documents are continually added, TF-IDF scores can become stale unless recalculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad5643d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>at</th>\n",
       "      <th>ball</th>\n",
       "      <th>barked</th>\n",
       "      <th>cat</th>\n",
       "      <th>chased</th>\n",
       "      <th>dog</th>\n",
       "      <th>mat</th>\n",
       "      <th>mouse</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471578</td>\n",
       "      <td>0.471578</td>\n",
       "      <td>0.492178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.492768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492768</td>\n",
       "      <td>0.314527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361459</td>\n",
       "      <td>0.446473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431887</td>\n",
       "      <td>0.431887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         at      ball    barked       cat    chased       dog       mat  \\\n",
       "0  0.000000  0.000000  0.000000  0.301002  0.000000  0.000000  0.471578   \n",
       "1  0.492768  0.000000  0.492768  0.314527  0.000000  0.388504  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.361459  0.446473  0.000000  0.000000   \n",
       "3  0.000000  0.547794  0.000000  0.000000  0.431887  0.431887  0.000000   \n",
       "\n",
       "      mouse        on       sat       the  \n",
       "0  0.000000  0.471578  0.471578  0.492178  \n",
       "1  0.000000  0.000000  0.000000  0.514293  \n",
       "2  0.566295  0.000000  0.000000  0.591032  \n",
       "3  0.000000  0.000000  0.000000  0.571724  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog barked at the cat\",\n",
    "    \"The cat chased the mouse\",\n",
    "    \"The dog chased the ball\"\n",
    "]\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus into TF-IDF representation\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the result to a DataFrame for readability\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "print(\"TF-IDF Representation:\")\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd897da1",
   "metadata": {},
   "source": [
    "## Word Embeddings  \n",
    "\n",
    "Word embeddings are a type of dense vector representation where words with similar meanings have similar vector representations. Instead of treating words as discrete entities, embeddings place them in a continuous vector space, allowing models to capture semantic relationships and contextual similarity. \n",
    "\n",
    "For instance, in a well-trained word embedding model, the relationship \"king - man + woman\" could approximate the vector for \"queen,\" showing how embeddings can encode relational knowledge.\n",
    "\n",
    "Word embeddings are typically generated by training on large corpora of text. The model learns to place words in the vector space such that words that appear in similar contexts have similar vector representations. The training process focuses on capturing semantic relationships and dependencies between words.\n",
    "\n",
    "Popular methods for generating word embeddings include **Word2Vec**, **GloVe**, and **FastText**. These models learn from large corpora and capture relationships between words, such as analogies and semantic similarities.\n",
    "\n",
    "Word embeddings overcome many limitations of simpler methods like BoW and TF-IDF by capturing semantic meaning and some degree of context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf2e4a",
   "metadata": {},
   "source": [
    "## Word2Vec  \n",
    "\n",
    "Word2Vec is a popular algorithm for generating word embeddings. Word2Vec uses neural networks to map words into a continuous vector space based on their context in a corpus of text. The key idea is that words appearing in similar contexts will have similar vector representations.  \n",
    "\n",
    "The training objective of Word2Vec is to predict either:\n",
    "- the surrounding words given a target word (Skip-Gram) \n",
    "- or predict the target word given its surrounding words (CBOW). \n",
    "  \n",
    "**Skip-Gram:**\n",
    "- The model looks at a single “center” word and tries to guess which words appear around it in the sentence. For example, if the center word is “dog,” it will learn to predict words like “barks,” “leash,” or “park” if those tend to appear near “dog.”\n",
    "\n",
    "**CBOW (Continuous Bag of Words):**\n",
    "- The model does the reverse. It takes a group of words around a blank spot (the context) and tries to predict which word belongs in that position. For instance, seeing “the ___ chased the ball,” it would learn to predict “dog.”\n",
    "\n",
    "Because it only needs to pay attention to a few words at a time rather than the entire vocabulary, Word2Vec can process really large amounts of text very quickly. The end result is that words with similar meanings or grammar roles (like “run” and “jog,” or “cat” and “kitty”) get mapped to vectors that sit close together in this new numerical space. These vectors then become convenient features for tasks like classifying documents, finding related words, or powering simple search systems.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1200/1*xC6wfTU_zpUlpRlXs5NZ4w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab6046",
   "metadata": {},
   "source": [
    "### Skip-Gram Model  \n",
    "\n",
    "The Skip-Gram model, a core component of Word2Vec, predicts the surrounding context words given a target word. For example, given the word \"cat,\" the model will try to predict nearby words like \"the,\" \"sat,\" or \"mat.\"  \n",
    "\n",
    "Mathematically, this looks like:  \n",
    "\n",
    "$$\n",
    "\\text{maximize} \\, \\sum \\log P(\\text{context} \\mid \\text{target})\n",
    "$$\n",
    "\n",
    "Skip-Gram is particularly useful for capturing meaningful relationships when the corpus is large, as it can effectively handle rare words. One way to understand Skip-Gram intuitively is to visualize a sliding window moving over the text and predicting the words within the window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e215ab",
   "metadata": {},
   "source": [
    "### Continuous Bag of Words (CBOW)  \n",
    "\n",
    "CBOW is the opposite of the Skip-Gram model. Instead of predicting context words given a target word, it predicts the target word based on its surrounding context. Given a window of words surrounding a target word, the model aims to predict the target word.\n",
    "\n",
    "Mathematically, this looks like:\n",
    "\n",
    "$$\n",
    "\\text{maximize} \\, \\sum \\log P(\\text{target} \\mid \\text{context})\n",
    "$$  \n",
    "\n",
    "CBOW is generally faster to train than Skip-Gram because it averages the context word vectors to predict the target word. However, it may not perform as well on rare words since it relies heavily on the surrounding context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39a662",
   "metadata": {},
   "source": [
    "### Coding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20623998",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b01926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat':\n",
      "[-0.01753848  0.00737714  0.01058508  0.01203614  0.01473678 -0.01264185\n",
      "  0.00283708  0.01270612 -0.00632933 -0.01250195 -0.00059064 -0.01700857\n",
      " -0.01139127  0.01432026  0.00677256  0.01453993  0.01423641  0.01551222\n",
      " -0.00832898 -0.00157581  0.00498176 -0.00899076  0.01758184 -0.01981414\n",
      "  0.01403708  0.00569289 -0.00990202  0.00932641 -0.00395903  0.01334891\n",
      "  0.0198663  -0.00912277 -0.00077523 -0.01217308  0.00750499  0.00543842\n",
      "  0.01447675  0.01197266  0.01928601  0.01860449  0.0156456  -0.01382573\n",
      " -0.01894968 -0.00101606 -0.0058394   0.01620697  0.01174434 -0.00293944\n",
      "  0.00312232  0.00403298]\n",
      "\n",
      "Most similar words to 'cat':\n",
      "on: 0.18660055100917816\n",
      "the: 0.16965018212795258\n",
      "mat: 0.16849403083324432\n",
      "\n",
      "Similarity between 'cat' and 'dog': 0.03\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus (list of tokenized sentences)\n",
    "corpus = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"barked\", \"at\", \"the\", \"cat\"],\n",
    "    [\"the\", \"dog\", \"chased\", \"the\", \"ball\"],\n",
    "    [\"the\", \"cat\", \"chased\", \"the\", \"mouse\"],\n",
    "    [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"sofa\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model - Skip-Gram Model\n",
    "model = Word2Vec(sentences=corpus, vector_size=50, window=3, min_count=1, sg=1, epochs=100)\n",
    "\n",
    "# Train Word2Vec model - CBOW Model\n",
    "#model = Word2Vec(sentences=corpus, vector_size=50, window=1, min_count=1, sg=0, epochs=100)\n",
    "\n",
    "# Save the trained model\n",
    "# model.save(\"word2vec.model\")\n",
    "\n",
    "# Access vector for a word\n",
    "cat_vector = model.wv[\"cat\"]\n",
    "print(f\"Vector for 'cat':\\n{cat_vector}\")\n",
    "\n",
    "# Find the most similar words to 'cat'\n",
    "similar_words = model.wv.most_similar(\"cat\", topn=3)\n",
    "print(\"\\nMost similar words to 'cat':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")\n",
    "\n",
    "# Find similarity between two words\n",
    "similarity_score = model.wv.similarity(\"cat\", \"dog\")\n",
    "print(f\"\\nSimilarity between 'cat' and 'dog': {similarity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212c962",
   "metadata": {},
   "source": [
    "#### Explanation of Parameters:\n",
    "\n",
    "- **`sentences`**: The corpus of tokenized sentences used for training.  \n",
    "- **`vector_size=50`**: The size (dimensionality) of the word embeddings.  \n",
    "- **`window=3`**: The context window size, meaning the model looks at 3 words before and after the target word.  \n",
    "- **`min_count=1`**: Minimum number of occurrences for a word to be included in the vocabulary.  \n",
    "- **`sg=1`**: Skip-Gram model (set `sg=0` for CBOW).  \n",
    "- **`epochs=100`**: Number of training epochs (iterations over the corpus).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015817c",
   "metadata": {},
   "source": [
    "## Alternatives to Word2Vec  \n",
    "\n",
    "Although Word2Vec is widely used, several alternative methods exist that address some of its limitations or take different approaches to learning word representations. In this section, we will explore two important alternatives:\n",
    "\n",
    "- **GloVe (Global Vectors for Word Representation):** A method that leverages global co-occurrence statistics rather than local context windows.\n",
    "- **BERT and Contextual Embeddings:** Models that generate dynamic word representations based on the surrounding context, allowing the same word to have different embeddings depending on usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rj55ddhnjd",
   "metadata": {},
   "source": [
    "### GloVe (Global Vectors for Word Representation)\n",
    "\n",
    "GloVe is an unsupervised learning algorithm developed by Stanford researchers that generates word embeddings by leveraging global word-word co-occurrence statistics from a corpus. Unlike Word2Vec, which learns embeddings by predicting context words from a sliding window, GloVe constructs a large co-occurrence matrix that captures how frequently words appear together across the entire corpus.\n",
    "\n",
    "#### How GloVe Works\n",
    "\n",
    "1. **Co-occurrence Matrix:** GloVe first builds a matrix $X$ where each entry $X_{ij}$ represents how often word $i$ appears in the context of word $j$ across the entire corpus.\n",
    "\n",
    "2. **Weighted Least Squares Objective:** The model then learns word vectors by minimizing a weighted least squares objective that tries to encode the relationship between co-occurrence probabilities:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i,j=1}^{V} f(X_{ij}) \\left( w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $w_i$ and $\\tilde{w}_j$ are word vectors for words $i$ and $j$\n",
    "- $b_i$ and $\\tilde{b}_j$ are bias terms\n",
    "- $f(X_{ij})$ is a weighting function that prevents very frequent co-occurrences from dominating the objective\n",
    "\n",
    "3. **Final Embeddings:** The final word embedding for a word is typically the sum of its $w$ and $\\tilde{w}$ vectors.\n",
    "\n",
    "#### Key Differences from Word2Vec\n",
    "\n",
    "| Aspect | Word2Vec | GloVe |\n",
    "|--------|----------|-------|\n",
    "| Training approach | Predictive (neural network) | Count-based (matrix factorization) |\n",
    "| Context | Local (sliding window) | Global (entire corpus statistics) |\n",
    "| Objective | Predict context/target words | Reconstruct co-occurrence log-probabilities |\n",
    "\n",
    "#### Advantages of GloVe\n",
    "- Efficiently leverages global statistical information\n",
    "- Often produces high-quality embeddings with less training time on large corpora\n",
    "- Better at capturing global semantic relationships\n",
    "\n",
    "#### Coding Example\n",
    "\n",
    "We will use pre-trained GloVe vectors, which are commonly available and widely used in practice. The `gensim` library provides an easy way to load and work with these embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sntha6cutgd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat' (first 10 dimensions):\n",
      "[ 0.23088   0.28283   0.6318   -0.59411  -0.58599   0.63255   0.24402\n",
      " -0.14108   0.060815 -0.7898  ]\n",
      "\n",
      "Vector dimensionality: 100\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe vectors (this may take a moment to download)\n",
    "# glove-wiki-gigaword-100 contains 100-dimensional vectors trained on Wikipedia and Gigaword\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Access the vector for a word\n",
    "cat_vector = glove_model[\"cat\"]\n",
    "print(f\"Vector for 'cat' (first 10 dimensions):\\n{cat_vector[:10]}\")\n",
    "print(f\"\\nVector dimensionality: {len(cat_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1339io3ww0w",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'computer':\n",
      "computers: 0.8752\n",
      "software: 0.8373\n",
      "technology: 0.7642\n",
      "pc: 0.7366\n",
      "hardware: 0.7290\n"
     ]
    }
   ],
   "source": [
    "# Find the most similar words to 'computer'\n",
    "similar_words = glove_model.most_similar(\"computer\", topn=5)\n",
    "print(\"Most similar words to 'computer':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oxds5uvdvgb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman = ?\n",
      "queen: 0.7699\n",
      "monarch: 0.6843\n",
      "throne: 0.6756\n"
     ]
    }
   ],
   "source": [
    "# Word analogies: king - man + woman = ?\n",
    "# GloVe embeddings capture semantic relationships that allow for analogy reasoning\n",
    "result = glove_model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=3)\n",
    "print(\"king - man + woman = ?\")\n",
    "for word, similarity in result:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7wxxh914ofm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'computer' and 'software': 0.8373\n",
      "Similarity between 'computer' and 'banana': 0.1213\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity between two words\n",
    "similarity_score = glove_model.similarity(\"computer\", \"software\")\n",
    "print(f\"Similarity between 'computer' and 'software': {similarity_score:.4f}\")\n",
    "\n",
    "# Compare with unrelated words\n",
    "similarity_score_unrelated = glove_model.similarity(\"computer\", \"banana\")\n",
    "print(f\"Similarity between 'computer' and 'banana': {similarity_score_unrelated:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13w1ooez6nt",
   "metadata": {},
   "source": [
    "### BERT and Contextual Embeddings\n",
    "\n",
    "Unlike Word2Vec and GloVe, which produce **static embeddings** (the same vector for a word regardless of context), **BERT (Bidirectional Encoder Representations from Transformers)** generates **contextual embeddings**. This means the same word can have different vector representations depending on its surrounding words.\n",
    "\n",
    "For example, consider the word \"bank\":\n",
    "- \"I deposited money at the **bank**\" (financial institution)\n",
    "- \"I sat by the river **bank**\" (edge of a river)\n",
    "\n",
    "With static embeddings, \"bank\" would have a single vector. With BERT, \"bank\" gets different embeddings in each sentence, capturing its distinct meanings.\n",
    "\n",
    "#### Key Differences from Static Embeddings\n",
    "\n",
    "| Aspect | Static Embeddings (Word2Vec, GloVe) | Contextual Embeddings (BERT) |\n",
    "|--------|-------------------------------------|------------------------------|\n",
    "| Word representation | One vector per word | Different vectors based on context |\n",
    "| Polysemy handling | Single representation for all meanings | Distinct representations for different meanings |\n",
    "| Computational cost | Low (lookup table) | High (forward pass through model) |\n",
    "\n",
    "We will explore the Transformer architecture that powers BERT in more detail later in this course. For now, let's see how to use BERT to generate contextual embeddings.\n",
    "\n",
    "#### Coding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ksrnvpc8ash",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: torch in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (1.12.1)\n",
      "Requirement already satisfied: filelock in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jonathanschlosser/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nru5i2014dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'the', 'cat', 'sat', 'on', 'the', 'mat', '[SEP]']\n",
      "Embedding shape: torch.Size([1, 8, 768])\n",
      "\n",
      "Embedding for 'cat' (first 10 dimensions):\n",
      "tensor([-0.3106, -0.0439,  0.2795, -0.1275,  0.5369,  0.0781,  0.2070,  0.4593,\n",
      "         0.2091, -0.3197])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# Example: Get embedding for a word in context\n",
    "sentence = \"The cat sat on the mat\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "\n",
    "# Get BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Shape: (batch_size, sequence_length, hidden_size=768)\n",
    "embeddings = outputs.last_hidden_state\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "\n",
    "# Get the embedding for \"cat\" (index 2)\n",
    "cat_embedding = embeddings[0, 2, :]\n",
    "print(f\"\\nEmbedding for 'cat' (first 10 dimensions):\\n{cat_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40lixdtdt8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I deposited money at the bank' vs 'I sat by the river bank'\n",
      "Cosine similarity between 'bank' embeddings: 0.7393\n",
      "\n",
      "Lower similarity shows BERT recognizes these are different uses of 'bank'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_word_embedding(sentence, word):\n",
    "    \"\"\"Extract the BERT embedding for a specific word in a sentence.\"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    word_index = tokens.index(word.lower())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[0, word_index, :].numpy()\n",
    "\n",
    "# \"bank\" in different contexts\n",
    "sentence1 = \"I deposited money at the bank\"\n",
    "sentence2 = \"I sat by the river bank\"\n",
    "\n",
    "bank_financial = get_word_embedding(sentence1, \"bank\")\n",
    "bank_river = get_word_embedding(sentence2, \"bank\")\n",
    "\n",
    "similarity = cosine_similarity([bank_financial], [bank_river])[0][0]\n",
    "print(f\"'{sentence1}' vs '{sentence2}'\")\n",
    "print(f\"Cosine similarity between 'bank' embeddings: {similarity:.4f}\")\n",
    "print(\"\\nLower similarity shows BERT recognizes these are different uses of 'bank'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "t5u55dy2x8m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I deposited money at the bank' vs 'I withdrew cash from the bank'\n",
      "Cosine similarity (same sense): 0.9458\n",
      "\n",
      "Higher similarity when 'bank' is used in the same sense\n"
     ]
    }
   ],
   "source": [
    "# Compare: \"bank\" used in the same sense (both financial)\n",
    "sentence3 = \"I withdrew cash from the bank\"\n",
    "bank_financial_2 = get_word_embedding(sentence3, \"bank\")\n",
    "\n",
    "similarity_same = cosine_similarity([bank_financial], [bank_financial_2])[0][0]\n",
    "print(f\"'{sentence1}' vs '{sentence3}'\")\n",
    "print(f\"Cosine similarity (same sense): {similarity_same:.4f}\")\n",
    "print(\"\\nHigher similarity when 'bank' is used in the same sense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ce7db",
   "metadata": {},
   "source": [
    "## Important Things to Consider  \n",
    "\n",
    "#### Window Size  \n",
    "The window size defines the number of words around a target word that the model should consider when learning embeddings. A shorter window (e.g., 2-3 words) typically captures **syntactic** information, such as word order or grammatical relationships. A longer window (e.g., 5-10 words) is better for capturing **semantic or topical relationships** between words.  \n",
    "\n",
    "For example:  \n",
    "- A shorter window size may learn that \"run\" and \"ran\" have similar grammatical functions. \n",
    "- A longer window size may capture that \"book\" and \"novel\" are topically related, even if they are not grammatically similar.  \n",
    "  \n",
    "#### Subword Information  \n",
    "Languages with complex morphology or compound words benefit from embeddings that capture subword information. Techniques like FastText and character-based embeddings decompose words into smaller units, enabling models to handle unseen or rare words more effectively.  \n",
    "#### Handling Out-of-Vocabulary (OOV) Words  \n",
    "Static embeddings, such as Word2Vec or GloVe, suffer from the issue of out-of-vocabulary (OOV) words. If a word is not in the training corpus, it will not have an embedding. Contextual embeddings and subword-based models provide solutions to this problem by dynamically generating embeddings or breaking words into subunits.  \n",
    "\n",
    "#### Semantic and Syntactic Trade-offs  \n",
    "Depending on the task, you may need embeddings that prioritize semantic relationships (e.g., grouping words by topic) or syntactic relationships (e.g., grammatical roles). Tuning parameters like window size, corpus size, and embedding dimensions allows for optimizing the model based on your needs.\n",
    "\n",
    "#### Dimensionality and Training Time  \n",
    "Choosing the dimensionality of the word vectors is important. Larger dimensions generally result in better performance but increase computational cost and risk overfitting. Practical implementations balance the quality of embeddings with the available computational resources.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980262f",
   "metadata": {},
   "source": [
    "## Distance Functions\n",
    "\n",
    "When we represent text as vectors in an **n-dimensional space**, comparing the similarity between different text representations involves calculating the \"distance\" between them. \n",
    "\n",
    "These distances provide a quantitative measure of how similar (or different) two text samples are, which is crucial for tasks like document clustering, classification, and information retrieval.\n",
    "\n",
    "The choice of distance function can impact the performance of your model, as different tasks may require different types of similarity measures. \n",
    "\n",
    "![](https://www.maartengrootendorst.com/images/posts/2021-01-02-distances/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d0661",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "Euclidean distance is one of the most common and straightforward distance metrics. It measures the straight-line distance between two points in space. \n",
    "\n",
    "Given two vectors $ \\mathbf{A} $ and $ \\mathbf{B} $, the Euclidean distance $ d $ is defined as:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{A}, \\mathbf{B}) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}\n",
    "$$\n",
    "\n",
    "Intuitively, this works well when comparing vectors based on magnitude or when word order does not matter. However, in high-dimensional spaces or sparse text data, Euclidean distance may not always capture meaningful similarities.\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0757ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance (manual): 3.605551275463989\n",
      "Euclidean distance (scipy): 3.605551275463989\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Define two vectors (representing text vectors or points in n-dimensional space)\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 0, 3])\n",
    "\n",
    "# Method 1: Manual computation of Euclidean distance\n",
    "euclidean_manual = np.sqrt(np.sum((vector1 - vector2)**2))\n",
    "print(f\"Euclidean distance (manual): {euclidean_manual}\")\n",
    "\n",
    "# Method 2: Using scipy's euclidean function\n",
    "euclidean_scipy = euclidean(vector1, vector2)\n",
    "print(f\"Euclidean distance (scipy): {euclidean_scipy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea3df7",
   "metadata": {},
   "source": [
    "### Manhattan Distance (L1 Distance)\n",
    "\n",
    "Manhattan distance measures the distance by summing the absolute differences between the vector components. \n",
    "\n",
    "Mathematically, it is given by:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{A}, \\mathbf{B}) = \\sum_{i=1}^{n} |A_i - B_i|\n",
    "$$\n",
    "\n",
    "This metric is useful when movement between dimensions is restricted. It is often used in text-related tasks where small deviations in feature values are meaningful.\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e4443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan distance (manual): 5\n",
      "Manhattan distance (scipy): 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cityblock  # Scipy function for Manhattan distance\n",
    "\n",
    "# Define two vectors (representing text vectors or points in n-dimensional space)\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 0, 3])\n",
    "\n",
    "# Method 1: Manual computation of Manhattan distance\n",
    "manhattan_manual = np.sum(np.abs(vector1 - vector2))\n",
    "print(f\"Manhattan distance (manual): {manhattan_manual}\")\n",
    "\n",
    "# Method 2: Using scipy's cityblock function\n",
    "manhattan_scipy = cityblock(vector1, vector2)\n",
    "print(f\"Manhattan distance (scipy): {manhattan_scipy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9a9d8",
   "metadata": {},
   "source": [
    "### Minkowski Distance\n",
    "\n",
    "The **Minkowski distance** is a generalized form of distance that includes both the **Euclidean distance** and **Manhattan distance** as special cases. The formula for the Minkowski distance between two vectors $ \\mathbf{A} $ and $ \\mathbf{B} $ is:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{A}, \\mathbf{B}) = \\left( \\sum_{i=1}^{n} |A_i - B_i|^p \\right)^{\\frac{1}{p}}\n",
    "$$\n",
    "\n",
    "- **p = 1:** The formula becomes the **Manhattan distance**.  \n",
    "- **p = 2:** The formula becomes the **Euclidean distance**.  \n",
    "- For larger values of $ p $, the metric places more emphasis on large differences between vector components.  \n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfbe59f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minkowski distance (manual, p=3): 3.2710663101885897\n",
      "Minkowski distance (scipy, p=3): 3.2710663101885897\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import minkowski\n",
    "\n",
    "# Define two vectors (representing text vectors or points in n-dimensional space)\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 0, 3])\n",
    "\n",
    "# Method 1: Manual computation of Minkowski distance with p = 3\n",
    "p = 3\n",
    "minkowski_manual = np.power(np.sum(np.abs(vector1 - vector2)**p), 1/p)\n",
    "print(f\"Minkowski distance (manual, p=3): {minkowski_manual}\")\n",
    "\n",
    "# Method 2: Using scipy's minkowski function\n",
    "minkowski_scipy = minkowski(vector1, vector2, p=3)\n",
    "print(f\"Minkowski distance (scipy, p=3): {minkowski_scipy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c5f93",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "Cosine similarity is a popular measure for text representations because it compares the **angles between vectors** rather than their magnitudes. This makes it ideal for text data, where two documents with different lengths may still have high similarity if they share the same word distribution.\n",
    "\n",
    "The formula for cosine similarity between two vectors $ \\mathbf{A} $ and $ \\mathbf{B} $ is:\n",
    "\n",
    "$$\n",
    "\\text{cosine similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\n",
    "$$\n",
    "\n",
    "Here, $ \\mathbf{A} \\cdot \\mathbf{B} $ represents the dot product of the two vectors, and $ \\|\\mathbf{A}\\| $ and $ \\|\\mathbf{B}\\| $ represent their magnitudes.\n",
    "\n",
    "Cosine similarity is widely used in **information retrieval** and **document clustering** because it focuses on the direction of vectors, making it less sensitive to differences in vector length.\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c61bd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity (manual): 0.6948792289723034\n",
      "Cosine similarity (scikit-learn): 0.6948792289723034\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define two vectors (representing text vectors or points in n-dimensional space)\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 0, 3])\n",
    "\n",
    "# Method 1: Manual computation of cosine similarity\n",
    "dot_product = np.dot(vector1, vector2)\n",
    "magnitude1 = np.linalg.norm(vector1)\n",
    "magnitude2 = np.linalg.norm(vector2)\n",
    "cosine_similarity_manual = dot_product / (magnitude1 * magnitude2)\n",
    "print(f\"Cosine similarity (manual): {cosine_similarity_manual}\")\n",
    "\n",
    "# Method 2: Using scikit-learn's cosine_similarity function\n",
    "cosine_similarity_sklearn = cosine_similarity([vector1], [vector2])[0][0]\n",
    "print(f\"Cosine similarity (scikit-learn): {cosine_similarity_sklearn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1bd10",
   "metadata": {},
   "source": [
    "### Other Distance Measures\n",
    "\n",
    "There are additional distance measures you may encounter depending on the specific problem:\n",
    "\n",
    "1. **Jaccard Similarity:** Measures the similarity between two sets by dividing the size of their intersection by the size of their union. This is particularly useful for comparing sparse vectors or binary representations of text.\n",
    "   $$\n",
    "   J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "   $$\n",
    "\n",
    "2. **Hamming Distance:** Measures the number of positions where corresponding components of two vectors differ. It is commonly used for comparing binary or categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fff317",
   "metadata": {},
   "source": [
    "## Real World Example Using Newsgroup Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111d651",
   "metadata": {},
   "source": [
    "#### Install Required Libraries (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93cf51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50da57",
   "metadata": {},
   "source": [
    "#### Load The Dataset\n",
    "\n",
    "We are using the 20 Newsgroups dataset, a common benchmark dataset in NLP. It contains around 18,000 newsgroup documents spread across 20 different categories, including topics like sports, politics, computers, and religion. This dataset is widely used for text classification, clustering, and word embedding demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eea77383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jonathanschlosser/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK tokenizer\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daea09a",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0047009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents into sentences of words\n",
    "sentences = [word_tokenize(doc.lower()) for doc in newsgroups_data.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "623fb7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'sure',\n",
       " 'some',\n",
       " 'bashers',\n",
       " 'of',\n",
       " 'pens',\n",
       " 'fans',\n",
       " 'are',\n",
       " 'pretty',\n",
       " 'confused',\n",
       " 'about',\n",
       " 'the',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'any',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'posts',\n",
       " 'about',\n",
       " 'the',\n",
       " 'recent',\n",
       " 'pens',\n",
       " 'massacre',\n",
       " 'of',\n",
       " 'the',\n",
       " 'devils',\n",
       " '.',\n",
       " 'actually',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " 'bit',\n",
       " 'puzzled',\n",
       " 'too',\n",
       " 'and',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'relieved',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'put',\n",
       " 'an',\n",
       " 'end',\n",
       " 'to',\n",
       " 'non-pittsburghers',\n",
       " \"'\",\n",
       " 'relief',\n",
       " 'with',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'praise',\n",
       " 'for',\n",
       " 'the',\n",
       " 'pens',\n",
       " '.',\n",
       " 'man',\n",
       " ',',\n",
       " 'they',\n",
       " 'are',\n",
       " 'killing',\n",
       " 'those',\n",
       " 'devils',\n",
       " 'worse',\n",
       " 'than',\n",
       " 'i',\n",
       " 'thought',\n",
       " '.',\n",
       " 'jagr',\n",
       " 'just',\n",
       " 'showed',\n",
       " 'you',\n",
       " 'why',\n",
       " 'he',\n",
       " 'is',\n",
       " 'much',\n",
       " 'better',\n",
       " 'than',\n",
       " 'his',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'stats',\n",
       " '.',\n",
       " 'he',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'fo',\n",
       " 'fun',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'in',\n",
       " 'the',\n",
       " 'playoffs',\n",
       " '.',\n",
       " 'bowman',\n",
       " 'should',\n",
       " 'let',\n",
       " 'jagr',\n",
       " 'have',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'fun',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'games',\n",
       " 'since',\n",
       " 'the',\n",
       " 'pens',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'beat',\n",
       " 'the',\n",
       " 'pulp',\n",
       " 'out',\n",
       " 'of',\n",
       " 'jersey',\n",
       " 'anyway',\n",
       " '.',\n",
       " 'i',\n",
       " 'was',\n",
       " 'very',\n",
       " 'disappointed',\n",
       " 'not',\n",
       " 'to',\n",
       " 'see',\n",
       " 'the',\n",
       " 'islanders',\n",
       " 'lose',\n",
       " 'the',\n",
       " 'final',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'game',\n",
       " '.',\n",
       " 'pens',\n",
       " 'rule',\n",
       " '!',\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fee7db",
   "metadata": {},
   "source": [
    "#### Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52b42c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model on the tokenized sentences\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4777b",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b031285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'computer':\n",
      "[-0.21225789  0.5404231   0.40459347  0.32228127  0.12945211 -0.3299833\n",
      " -0.09653363  0.20713666 -0.36554763  0.13997215 -0.37456098 -0.67297906\n",
      " -0.3721857   0.00911829 -0.20481554 -0.36107153  0.7215204   0.3608223\n",
      " -0.41200316 -0.7169939   0.38293985  0.86623466  0.09137827 -0.26366988\n",
      "  0.23594125 -0.17807429 -0.17314552 -0.07365485  0.2280913   0.200225\n",
      "  0.03429207 -0.2574355   0.28391963 -0.32798564 -0.03226035 -0.5596238\n",
      " -0.12443216  0.20995711  0.05276503 -0.3246789   0.9564842  -0.6072698\n",
      "  0.62925166  0.00243073  0.15336983  0.07485364 -0.31112248 -0.06012597\n",
      " -0.04015968  0.29611218 -0.20674986 -0.33931953 -0.40383413 -0.0755101\n",
      "  0.12370312  0.03053324 -0.13557634 -0.25636342 -0.26846898  0.263858\n",
      " -0.21728204  0.4428228   0.11869055  0.20396456 -0.00572924  0.39625686\n",
      " -0.02928978  0.41261107  0.07562852  0.1514651   0.10060371 -0.1340094\n",
      "  0.09531747 -0.27664697  0.58087593 -0.3628328  -0.00305403  0.30699235\n",
      " -0.08109504  0.03020633 -0.24945392  0.23120221 -0.13801555 -0.02945856\n",
      " -0.2450922  -0.3351114   0.04982343  0.5667882  -0.293193    0.30734646\n",
      "  0.33727098 -0.13578098  0.35929447  0.0221448  -0.13967526 -0.31036037\n",
      "  0.23958269 -0.74144065 -0.22419733 -0.24736337]\n"
     ]
    }
   ],
   "source": [
    "# Access the vector representation of a word\n",
    "word_vector = model.wv['computer']\n",
    "print(f\"Vector for 'computer':\\n{word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4a429aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar words to 'computer':\n",
      "visualisation: 0.6896\n",
      "graphics: 0.6778\n",
      "shopper: 0.6759\n",
      "eckton: 0.6675\n",
      "computing: 0.6632\n"
     ]
    }
   ],
   "source": [
    "# Find the most similar words to a given word\n",
    "similar_words = model.wv.most_similar('computer', topn=5)\n",
    "print(\"\\nMost similar words to 'computer':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "854f9ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity between 'computer' and 'software': 0.5553\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity between two words\n",
    "similarity_score = model.wv.similarity('computer', 'software')\n",
    "print(f\"\\nSimilarity between 'computer' and 'software': {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f10aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
