{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwatts519/Data-785-Deep-Learning/blob/main/Nakisha_BoulwareWatts_Programming_Assignment_Unit_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32wfeoVhQrPk"
      },
      "source": [
        "# Homework For Unit 2 - Logistic Regression\n",
        "\n",
        "In this assignment, you will learn how to implement binary logistic regression using numpy.\n",
        "\n",
        "## Task 1: Inspect and Plot the Data\n",
        "\n",
        "First, let's generate some realistic and noisy 2-dimensional data and plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lth3POv2QrPl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate some realistic and noisy 2D data\n",
        "np.random.seed(0)\n",
        "X = np.random.randn(100, 2)\n",
        "y = (X[:, 0] + X[:, 1] > 0).astype(np.float64)\n",
        "y += 0.1 * np.random.randn(100)  # add noise\n",
        "y = (y > 0.5).astype(np.float64)  # convert to binary labels\n",
        "\n",
        "# Add 5% outlier points\n",
        "num_outliers = int(0.05 * X.shape[0])\n",
        "outliers_X = np.random.uniform(low=-3, high=3, size=(num_outliers, 2))\n",
        "outliers_y = np.random.randint(0, 2, size=num_outliers).astype(np.float64)\n",
        "\n",
        "# Combine original data with outliers\n",
        "X = np.vstack([X, outliers_X])\n",
        "y = np.hstack([y, outliers_y])\n",
        "\n",
        "# Plot the data\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.title('Generated Data with Outliers')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsO72l-6QrPl"
      },
      "source": [
        "## Task 2: Implement Logistic Regression from Scratch\n",
        "\n",
        "Logistic regression is used for binary classification tasks. It models the probability that a given input belongs to a particular class.\n",
        "\n",
        "### Step 1: Initialize Parameters\n",
        "\n",
        "Let's start by initializing the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh8-PercQrPl"
      },
      "outputs": [],
      "source": [
        "# Initialize parameters\n",
        "theta = np.random.randn(3, 1)\n",
        "learning_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XUCP8WtQrPm"
      },
      "source": [
        "### Step 2: Compute the Sigmoid Function\n",
        "\n",
        "Implement the sigmoid function, which is used to map the predictions to probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuXD9iaXQrPm"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khHCKTG4QrPm"
      },
      "source": [
        "### Step 3: Compute the Prediction and Loss (2 pt)\n",
        "\n",
        "Implement the function to compute the predictions and the cross-entropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XImR_BXQrPm"
      },
      "outputs": [],
      "source": [
        "# Add x0 = 1 to each instance\n",
        "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "\n",
        "def predict(X, theta):\n",
        "    # TODO: Implement the predict function, which should return the sigmoids of the linear scores for rows in X .\n",
        "\n",
        "def compute_cross_entropy_loss(y, y_pred):\n",
        "    # TODO: Implement the cross entropy loss. Remember to normalize the loss by the number of examples (for example, use np.mean to avg the cross entropy across examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIO9D67YQrPm"
      },
      "source": [
        "### Step 4: Compute the Gradient (1 pt)\n",
        "\n",
        "Compute the gradient of the loss function with respect to the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UavKVd0gQrPm"
      },
      "outputs": [],
      "source": [
        "def compute_cross_entropy_gradient(X, y, y_pred):\n",
        "    # TODO: Compute and return the gradient of the cross entopy loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHs87Z0UQrPm"
      },
      "source": [
        "### Step 5: Train the Model using Gradient Descent\n",
        "\n",
        "Train the model using gradient descent and plot the loss over iterations. Come up with a stopping condition for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D3n0r1IQrPm"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_cross_entropy(X, y, theta, learning_rate, num_iterations, tol=1e-6):\n",
        "    loss_history = []\n",
        "    for i in range(num_iterations):\n",
        "        y_pred = predict(X, theta)\n",
        "        loss = compute_cross_entropy_loss(y, y_pred)\n",
        "        loss_history.append(loss)\n",
        "        gradient = compute_cross_entropy_gradient(X, y, y_pred)\n",
        "\n",
        "        print(f'Iteration {i+1}:')\n",
        "        print(f'Theta shape: {theta.shape}')\n",
        "        print(f'Gradient shape: {gradient.shape}')\n",
        "\n",
        "        theta -= learning_rate * gradient\n",
        "        if np.linalg.norm(gradient) < tol:\n",
        "            print(f'Stopping at iteration {i+1}')\n",
        "            break\n",
        "    return theta, loss_history\n",
        "\n",
        "# Train the model\n",
        "num_iterations = 1000\n",
        "theta, loss_history = gradient_descent_cross_entropy(X_b, y, theta, learning_rate, num_iterations)\n",
        "\n",
        "# Plot the cross-entropy loss over iterations\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Cross-Entropy Loss over Iterations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vpz0S07QrPm"
      },
      "source": [
        "### Step 6: Plot the Decision Boundary\n",
        "\n",
        "Plot the decision boundary obtained from gradient descent along with the original data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzZ-Uk38QrPm"
      },
      "outputs": [],
      "source": [
        "# Plot the decision boundary\n",
        "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.01),\n",
        "                       np.arange(x2_min, x2_max, 0.01))\n",
        "X_new = np.c_[np.ones((xx1.ravel().shape[0], 1)), xx1.ravel(), xx2.ravel()]\n",
        "y_predict = predict(X_new, theta)\n",
        "y_predict = y_predict.reshape(xx1.shape)\n",
        "\n",
        "plt.contourf(xx1, xx2, y_predict, alpha=0.8, cmap=plt.cm.Paired)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"X2\")\n",
        "plt.title(\"Logistic Regression Decision Boundary\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOcj59DuQrPm"
      },
      "source": [
        "## Task 3: Softmax Regression and Spurious Degree of Freedom\n",
        "\n",
        "Understand the spurious degree of freedom in softmax regression and show that binary logistic regression is a special case of softmax regression.\n",
        "\n",
        "### Step 1: Implement Softmax Function\n",
        "\n",
        "Implement the softmax function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rfbpVTGQrPm"
      },
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFLNhgL3QrPm"
      },
      "source": [
        "### Step 2: Show Spurious Degree of Freedom (1 pt)\n",
        "\n",
        "Show that adding the same scalar to all logits doesn't change the probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH3FvOjjQrPm"
      },
      "outputs": [],
      "source": [
        "# Define logits\n",
        "logits = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "# Compute softmax probabilities\n",
        "probs = softmax(logits)\n",
        "\n",
        "# Add a scalar value to all logits (TODO: try experimenting with different values of the scalar)\n",
        "scalar = 0.5\n",
        "logits_shifted = logits + scalar\n",
        "probs_shifted = softmax(logits_shifted)\n",
        "\n",
        "print(\"Original probabilities:\", probs)\n",
        "print(\"Shifted probabilities:\", probs_shifted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMb2zV-VQrPm"
      },
      "source": [
        "### Step 3: Binary Logistic Regression as a Reduction of Softmax Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCjpRB77QrPm"
      },
      "source": [
        "We know that the softmax function for the logits $z = [z_1, z_2]$ is defined as:\n",
        "$$ \\text{softmax}(z_i) = \\frac{e^{z_i}}{e^{z_1} + e^{z_2}} $$\n",
        "\n",
        "For binary classification, we can consider the logits $z = [0, z]$, where we set the first logit to 0 to remove the spurious degree of freedom.\n",
        "\n",
        "Thus, the softmax probabilities become:\n",
        "$$ \\text{softmax}(0, z) = \\left[ \\frac{e^0}{e^0 + e^z}, \\frac{e^z}{1 + e^z} \\right] $$\n",
        "\n",
        "Notice that the probability of the second class is:\n",
        "$$ \\frac{e^z}{1 + e^z} $$\n",
        "\n",
        "This is exactly the sigmoid function:\n",
        "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^z}{1 + e^z} $$\n",
        "\n",
        "Hence, we have shown that binary logistic regression using the sigmoid function is equivalent to multiclass (softmax) logistic regression when $K = 2$ classes."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}